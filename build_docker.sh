#!/bin/bash

# Build script for TinyLlama Docker images

set -e

echo "ğŸ³ Building TinyLlama Docker Images"
echo "==================================="

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "âŒ Docker is not running. Please start Docker Desktop first."
    exit 1
fi

echo ""
echo "ğŸ“‹ Available build options:"
echo "1. CPU-only image (smaller, works everywhere)"
echo "2. GPU-enabled image (CUDA support)"
echo "3. Both images"
echo ""

read -p "Choose option (1-3): " choice

case $choice in
    1)
        echo "ğŸ”¨ Building CPU-only image..."
        docker build -f Dockerfile.cpu -t tinyllama-api:cpu .
        echo "âœ… CPU image built successfully!"
        echo "ğŸš€ Run with: docker run -p 8000:8000 tinyllama-api:cpu"
        ;;
    2)
        echo "ğŸ”¨ Building GPU-enabled image..."
        docker build -f Dockerfile -t tinyllama-api:gpu .
        echo "âœ… GPU image built successfully!"
        echo "ğŸš€ Run with: docker run --gpus all -p 8000:8000 tinyllama-api:gpu"
        ;;
    3)
        echo "ğŸ”¨ Building both images..."
        echo "Building CPU image..."
        docker build -f Dockerfile.cpu -t tinyllama-api:cpu .
        echo "Building GPU image..."
        docker build -f Dockerfile -t tinyllama-api:gpu .
        echo "âœ… Both images built successfully!"
        echo "ğŸš€ CPU: docker run -p 8000:8000 tinyllama-api:cpu"
        echo "ğŸš€ GPU: docker run --gpus all -p 8000:8000 tinyllama-api:gpu"
        ;;
    *)
        echo "âŒ Invalid option"
        exit 1
        ;;
esac

echo ""
echo "ğŸ“¦ Available images:"
docker images | grep tinyllama-api

echo ""
echo "ğŸ’¡ Tips:"
echo "- The model is embedded in the image (~2.5GB total)"
echo "- No internet connection needed for inference"
echo "- Use docker-compose for easier deployment"
echo "- Test with: curl http://localhost:8000/health"